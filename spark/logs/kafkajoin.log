:: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-35702eca-2be4-4050-853a-9b769301b7c8;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 in central
	found org.apache.kafka#kafka-clients;2.8.0 in central
	found org.lz4#lz4-java;1.7.1 in central
	found org.xerial.snappy#snappy-java;1.1.8.4 in central
	found org.slf4j#slf4j-api;1.7.30 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.1 in central
	found org.spark-project.spark#unused;1.0.0 in central
	found org.apache.hadoop#hadoop-client-api;3.3.1 in central
	found org.apache.htrace#htrace-core4;4.1.0-incubating in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.6.2 in central
:: resolution report :: resolve 503ms :: artifacts dl 16ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.6.2 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]
	org.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]
	org.apache.kafka#kafka-clients;2.8.0 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 from central in [default]
	org.lz4#lz4-java;1.7.1 from central in [default]
	org.slf4j#slf4j-api;1.7.30 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.8.4 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-35702eca-2be4-4050-853a-9b769301b7c8
	confs: [default]
	0 artifacts copied, 13 already retrieved (0kB/14ms)
22/05/07 16:19:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/05/07 16:19:25 INFO SparkContext: Running Spark version 3.2.1
22/05/07 16:19:25 INFO ResourceUtils: ==============================================================
22/05/07 16:19:25 INFO ResourceUtils: No custom resources configured for spark.driver.
22/05/07 16:19:25 INFO ResourceUtils: ==============================================================
22/05/07 16:19:25 INFO SparkContext: Submitted application: customer-join
22/05/07 16:19:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
22/05/07 16:19:25 INFO ResourceProfile: Limiting resource is cpu
22/05/07 16:19:25 INFO ResourceProfileManager: Added ResourceProfile id: 0
22/05/07 16:19:25 INFO SecurityManager: Changing view acls to: spark
22/05/07 16:19:25 INFO SecurityManager: Changing modify acls to: spark
22/05/07 16:19:25 INFO SecurityManager: Changing view acls groups to: 
22/05/07 16:19:25 INFO SecurityManager: Changing modify acls groups to: 
22/05/07 16:19:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
22/05/07 16:19:25 INFO Utils: Successfully started service 'sparkDriver' on port 46289.
22/05/07 16:19:25 INFO SparkEnv: Registering MapOutputTracker
22/05/07 16:19:25 INFO SparkEnv: Registering BlockManagerMaster
22/05/07 16:19:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/05/07 16:19:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/05/07 16:19:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/05/07 16:19:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-df8ab095-045d-4ef2-bf64-39f90de6696d
22/05/07 16:19:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/05/07 16:19:25 INFO SparkEnv: Registering OutputCommitCoordinator
22/05/07 16:19:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/05/07 16:19:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ad5cda19b893:4040
22/05/07 16:19:25 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar at spark://ad5cda19b893:46289/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar with timestamp 1651940365096
22/05/07 16:19:25 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar at spark://ad5cda19b893:46289/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar with timestamp 1651940365096
22/05/07 16:19:25 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar at spark://ad5cda19b893:46289/jars/org.apache.kafka_kafka-clients-2.8.0.jar with timestamp 1651940365096
22/05/07 16:19:25 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://ad5cda19b893:46289/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1651940365096
22/05/07 16:19:25 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar at spark://ad5cda19b893:46289/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1651940365096
22/05/07 16:19:25 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://ad5cda19b893:46289/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1651940365096
22/05/07 16:19:25 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar at spark://ad5cda19b893:46289/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar with timestamp 1651940365096
22/05/07 16:19:25 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar at spark://ad5cda19b893:46289/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1651940365096
22/05/07 16:19:25 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at spark://ad5cda19b893:46289/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1651940365096
22/05/07 16:19:25 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar at spark://ad5cda19b893:46289/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1651940365096
22/05/07 16:19:25 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar at spark://ad5cda19b893:46289/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar with timestamp 1651940365096
22/05/07 16:19:25 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar at spark://ad5cda19b893:46289/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar with timestamp 1651940365096
22/05/07 16:19:25 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://ad5cda19b893:46289/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1651940365096
22/05/07 16:19:25 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar with timestamp 1651940365096
22/05/07 16:19:25 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar
22/05/07 16:19:25 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar with timestamp 1651940365096
22/05/07 16:19:25 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar
22/05/07 16:19:26 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.kafka_kafka-clients-2.8.0.jar
22/05/07 16:19:26 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/com.google.code.findbugs_jsr305-3.0.0.jar
22/05/07 16:19:26 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.commons_commons-pool2-2.6.2.jar
22/05/07 16:19:26 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.spark-project.spark_unused-1.0.0.jar
22/05/07 16:19:26 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar
22/05/07 16:19:26 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.lz4_lz4-java-1.7.1.jar
22/05/07 16:19:26 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.xerial.snappy_snappy-java-1.1.8.4.jar
22/05/07 16:19:26 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar at file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.slf4j_slf4j-api-1.7.30.jar
22/05/07 16:19:26 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.hadoop_hadoop-client-api-3.3.1.jar
22/05/07 16:19:26 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.htrace_htrace-core4-4.1.0-incubating.jar
22/05/07 16:19:26 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/commons-logging_commons-logging-1.1.3.jar
22/05/07 16:19:26 INFO Executor: Starting executor ID driver on host ad5cda19b893
22/05/07 16:19:26 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar
22/05/07 16:19:26 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/commons-logging_commons-logging-1.1.3.jar
22/05/07 16:19:26 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.kafka_kafka-clients-2.8.0.jar
22/05/07 16:19:26 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar
22/05/07 16:19:26 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.hadoop_hadoop-client-api-3.3.1.jar
22/05/07 16:19:26 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/com.google.code.findbugs_jsr305-3.0.0.jar
22/05/07 16:19:26 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.htrace_htrace-core4-4.1.0-incubating.jar
22/05/07 16:19:26 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.xerial.snappy_snappy-java-1.1.8.4.jar
22/05/07 16:19:26 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.commons_commons-pool2-2.6.2.jar
22/05/07 16:19:26 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.spark-project.spark_unused-1.0.0.jar
22/05/07 16:19:26 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.slf4j_slf4j-api-1.7.30.jar
22/05/07 16:19:26 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.lz4_lz4-java-1.7.1.jar
22/05/07 16:19:26 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar
22/05/07 16:19:26 INFO Executor: Fetching spark://ad5cda19b893:46289/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar with timestamp 1651940365096
22/05/07 16:19:26 INFO TransportClientFactory: Successfully created connection to ad5cda19b893/172.22.0.7:46289 after 61 ms (0 ms spent in bootstraps)
22/05/07 16:19:26 INFO Utils: Fetching spark://ad5cda19b893:46289/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp9040889223368271300.tmp
22/05/07 16:19:26 INFO Utils: /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp9040889223368271300.tmp has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar
22/05/07 16:19:27 INFO Executor: Adding file:/tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar to class loader
22/05/07 16:19:27 INFO Executor: Fetching spark://ad5cda19b893:46289/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1651940365096
22/05/07 16:19:27 INFO Utils: Fetching spark://ad5cda19b893:46289/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp6337233388366824076.tmp
22/05/07 16:19:27 INFO Utils: /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp6337233388366824076.tmp has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.spark-project.spark_unused-1.0.0.jar
22/05/07 16:19:27 INFO Executor: Adding file:/tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.spark-project.spark_unused-1.0.0.jar to class loader
22/05/07 16:19:27 INFO Executor: Fetching spark://ad5cda19b893:46289/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1651940365096
22/05/07 16:19:27 INFO Utils: Fetching spark://ad5cda19b893:46289/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp7211151691214137313.tmp
22/05/07 16:19:27 INFO Utils: /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp7211151691214137313.tmp has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/com.google.code.findbugs_jsr305-3.0.0.jar
22/05/07 16:19:27 INFO Executor: Adding file:/tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/com.google.code.findbugs_jsr305-3.0.0.jar to class loader
22/05/07 16:19:27 INFO Executor: Fetching spark://ad5cda19b893:46289/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar with timestamp 1651940365096
22/05/07 16:19:27 INFO Utils: Fetching spark://ad5cda19b893:46289/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp5567783456111624762.tmp
22/05/07 16:19:28 INFO Utils: /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp5567783456111624762.tmp has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.hadoop_hadoop-client-api-3.3.1.jar
22/05/07 16:19:28 INFO Executor: Adding file:/tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.hadoop_hadoop-client-api-3.3.1.jar to class loader
22/05/07 16:19:28 INFO Executor: Fetching spark://ad5cda19b893:46289/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1651940365096
22/05/07 16:19:28 INFO Utils: Fetching spark://ad5cda19b893:46289/jars/org.slf4j_slf4j-api-1.7.30.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp8291250086610347560.tmp
22/05/07 16:19:29 INFO Utils: /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp8291250086610347560.tmp has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.slf4j_slf4j-api-1.7.30.jar
22/05/07 16:19:29 INFO Executor: Adding file:/tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.slf4j_slf4j-api-1.7.30.jar to class loader
22/05/07 16:19:29 INFO Executor: Fetching spark://ad5cda19b893:46289/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1651940365096
22/05/07 16:19:29 INFO Utils: Fetching spark://ad5cda19b893:46289/jars/org.lz4_lz4-java-1.7.1.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp6888525807728709470.tmp
22/05/07 16:19:29 INFO Utils: /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp6888525807728709470.tmp has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.lz4_lz4-java-1.7.1.jar
22/05/07 16:19:29 INFO Executor: Adding file:/tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.lz4_lz4-java-1.7.1.jar to class loader
22/05/07 16:19:29 INFO Executor: Fetching spark://ad5cda19b893:46289/jars/org.apache.kafka_kafka-clients-2.8.0.jar with timestamp 1651940365096
22/05/07 16:19:29 INFO Utils: Fetching spark://ad5cda19b893:46289/jars/org.apache.kafka_kafka-clients-2.8.0.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp7293857270528459397.tmp
22/05/07 16:19:29 INFO Utils: /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp7293857270528459397.tmp has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.kafka_kafka-clients-2.8.0.jar
22/05/07 16:19:30 INFO Executor: Adding file:/tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.kafka_kafka-clients-2.8.0.jar to class loader
22/05/07 16:19:30 INFO Executor: Fetching spark://ad5cda19b893:46289/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1651940365096
22/05/07 16:19:30 INFO Utils: Fetching spark://ad5cda19b893:46289/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp472107360535118601.tmp
22/05/07 16:19:30 INFO Utils: /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp472107360535118601.tmp has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.xerial.snappy_snappy-java-1.1.8.4.jar
22/05/07 16:19:30 INFO Executor: Adding file:/tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.xerial.snappy_snappy-java-1.1.8.4.jar to class loader
22/05/07 16:19:30 INFO Executor: Fetching spark://ad5cda19b893:46289/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar with timestamp 1651940365096
22/05/07 16:19:30 INFO Utils: Fetching spark://ad5cda19b893:46289/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp4651689057189068273.tmp
22/05/07 16:19:30 INFO Utils: /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp4651689057189068273.tmp has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar
22/05/07 16:19:30 INFO Executor: Adding file:/tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar to class loader
22/05/07 16:19:30 INFO Executor: Fetching spark://ad5cda19b893:46289/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar with timestamp 1651940365096
22/05/07 16:19:30 INFO Utils: Fetching spark://ad5cda19b893:46289/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp8859795955458267513.tmp
22/05/07 16:19:30 INFO Utils: /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp8859795955458267513.tmp has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.htrace_htrace-core4-4.1.0-incubating.jar
22/05/07 16:19:30 INFO Executor: Adding file:/tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.htrace_htrace-core4-4.1.0-incubating.jar to class loader
22/05/07 16:19:30 INFO Executor: Fetching spark://ad5cda19b893:46289/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar with timestamp 1651940365096
22/05/07 16:19:30 INFO Utils: Fetching spark://ad5cda19b893:46289/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp5687245196770144965.tmp
22/05/07 16:19:30 INFO Utils: /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp5687245196770144965.tmp has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar
22/05/07 16:19:30 INFO Executor: Adding file:/tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar to class loader
22/05/07 16:19:30 INFO Executor: Fetching spark://ad5cda19b893:46289/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1651940365096
22/05/07 16:19:30 INFO Utils: Fetching spark://ad5cda19b893:46289/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp5721886054728331628.tmp
22/05/07 16:19:30 INFO Utils: /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp5721886054728331628.tmp has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/commons-logging_commons-logging-1.1.3.jar
22/05/07 16:19:30 INFO Executor: Adding file:/tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/commons-logging_commons-logging-1.1.3.jar to class loader
22/05/07 16:19:30 INFO Executor: Fetching spark://ad5cda19b893:46289/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1651940365096
22/05/07 16:19:30 INFO Utils: Fetching spark://ad5cda19b893:46289/jars/org.apache.commons_commons-pool2-2.6.2.jar to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp1752532159703937049.tmp
22/05/07 16:19:30 INFO Utils: /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/fetchFileTemp1752532159703937049.tmp has been previously copied to /tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.commons_commons-pool2-2.6.2.jar
22/05/07 16:19:30 INFO Executor: Adding file:/tmp/spark-dcf4aa4d-5b4d-4267-907b-7396a17b3993/userFiles-23d5476a-5f58-416a-b80f-c959a6a39f0e/org.apache.commons_commons-pool2-2.6.2.jar to class loader
22/05/07 16:19:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46131.
22/05/07 16:19:30 INFO NettyBlockTransferService: Server created on ad5cda19b893:46131
22/05/07 16:19:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/05/07 16:19:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ad5cda19b893, 46131, None)
22/05/07 16:19:30 INFO BlockManagerMasterEndpoint: Registering block manager ad5cda19b893:46131 with 366.3 MiB RAM, BlockManagerId(driver, ad5cda19b893, 46131, None)
22/05/07 16:19:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ad5cda19b893, 46131, None)
22/05/07 16:19:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ad5cda19b893, 46131, None)
22/05/07 16:19:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
22/05/07 16:19:31 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
22/05/07 16:19:36 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
22/05/07 16:19:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-5f27d62e-881e-4f1b-a47f-b9e4cbc69c6b-670584047-driver-0-1, groupId=spark-kafka-source-5f27d62e-881e-4f1b-a47f-b9e4cbc69c6b-670584047-driver-0] Error while fetching metadata with correlation id 2 : {stedi-events=LEADER_NOT_AVAILABLE}
22/05/07 16:19:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-5f27d62e-881e-4f1b-a47f-b9e4cbc69c6b-670584047-driver-0-1, groupId=spark-kafka-source-5f27d62e-881e-4f1b-a47f-b9e4cbc69c6b-670584047-driver-0] Error while fetching metadata with correlation id 7 : {stedi-events=LEADER_NOT_AVAILABLE}
^CERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py", line 475, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=3>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py", line 503, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py", line 475, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/opt/bitnami/python/lib/python3.8/socket.py", line 669, in readinto
    return self._sock.recv_into(b)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 292, in signal_handler
    self.cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 1195, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1321, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o14.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py", line 503, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
Traceback (most recent call last):
  File "/home/workspace/pyspark_kafka_join/sparkpykafkajoin.py", line 210, in <module>
    final_df \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py", line 101, in awaitTermination
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 334, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling o137.awaitTermination
22/05/07 16:29:46 ERROR Inbox: Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@58a3b182 rejected from java.util.concurrent.ThreadPoolExecutor@551f709f[Shutting down, pool size = 15, active threads = 15, queued tasks = 0, completed tasks = 19647]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:46 ERROR Inbox: Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@41e57b5f rejected from java.util.concurrent.ThreadPoolExecutor@551f709f[Shutting down, pool size = 14, active threads = 14, queued tasks = 0, completed tasks = 19648]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:46 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@6f66b9c6 is aborting.
22/05/07 16:29:46 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@6f66b9c6 aborted.
22/05/07 16:29:46 ERROR Inbox: Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@1c0f9b15 rejected from java.util.concurrent.ThreadPoolExecutor@551f709f[Shutting down, pool size = 13, active threads = 11, queued tasks = 0, completed tasks = 19651]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:46 ERROR Inbox: Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@707797fa rejected from java.util.concurrent.ThreadPoolExecutor@551f709f[Shutting down, pool size = 13, active threads = 11, queued tasks = 0, completed tasks = 19651]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:46 ERROR Inbox: Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@1927dadc rejected from java.util.concurrent.ThreadPoolExecutor@551f709f[Shutting down, pool size = 13, active threads = 11, queued tasks = 0, completed tasks = 19651]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:46 ERROR Utils: Aborting task
org.apache.spark.executor.CommitDeniedException: Commit denied for partition 75 (task 19652, attempt 0, stage 293.0)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:46 ERROR Utils: Aborting task
org.apache.spark.executor.CommitDeniedException: Commit denied for partition 70 (task 19647, attempt 0, stage 293.0)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:46 ERROR Utils: Aborting task
org.apache.spark.executor.CommitDeniedException: Commit denied for partition 79 (task 19656, attempt 0, stage 293.0)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:46 ERROR DataWritingSparkTask: Aborting commit for partition 70 (task 19647, attempt 0, stage 293.0)
22/05/07 16:29:46 ERROR DataWritingSparkTask: Aborted commit for partition 70 (task 19647, attempt 0, stage 293.0)
22/05/07 16:29:46 ERROR DataWritingSparkTask: Aborting commit for partition 79 (task 19656, attempt 0, stage 293.0)
22/05/07 16:29:46 ERROR DataWritingSparkTask: Aborting commit for partition 75 (task 19652, attempt 0, stage 293.0)
22/05/07 16:29:46 ERROR DataWritingSparkTask: Aborted commit for partition 79 (task 19656, attempt 0, stage 293.0)
22/05/07 16:29:46 ERROR DataWritingSparkTask: Aborted commit for partition 75 (task 19652, attempt 0, stage 293.0)
22/05/07 16:29:46 ERROR Utils: Aborting task
org.apache.spark.executor.CommitDeniedException: Commit denied for partition 77 (task 19654, attempt 0, stage 293.0)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:46 ERROR DataWritingSparkTask: Aborting commit for partition 77 (task 19654, attempt 0, stage 293.0)
22/05/07 16:29:46 ERROR DataWritingSparkTask: Aborted commit for partition 77 (task 19654, attempt 0, stage 293.0)
22/05/07 16:29:47 ERROR Utils: Aborting task
org.apache.spark.executor.CommitDeniedException: Commit denied for partition 78 (task 19655, attempt 0, stage 293.0)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:47 ERROR DataWritingSparkTask: Aborting commit for partition 78 (task 19655, attempt 0, stage 293.0)
22/05/07 16:29:47 ERROR DataWritingSparkTask: Aborted commit for partition 78 (task 19655, attempt 0, stage 293.0)
22/05/07 16:29:47 ERROR Inbox: Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@20500da7 rejected from java.util.concurrent.ThreadPoolExecutor@551f709f[Shutting down, pool size = 7, active threads = 6, queued tasks = 0, completed tasks = 19656]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:47 ERROR Inbox: Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@48a747b6 rejected from java.util.concurrent.ThreadPoolExecutor@551f709f[Shutting down, pool size = 6, active threads = 6, queued tasks = 0, completed tasks = 19656]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:47 ERROR Inbox: Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@5de2b3df rejected from java.util.concurrent.ThreadPoolExecutor@551f709f[Shutting down, pool size = 6, active threads = 6, queued tasks = 0, completed tasks = 19656]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:47 ERROR Inbox: Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@1558d914 rejected from java.util.concurrent.ThreadPoolExecutor@551f709f[Shutting down, pool size = 6, active threads = 6, queued tasks = 0, completed tasks = 19656]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:47 ERROR Inbox: Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@196b7341 rejected from java.util.concurrent.ThreadPoolExecutor@551f709f[Shutting down, pool size = 6, active threads = 6, queued tasks = 0, completed tasks = 19656]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:47 ERROR Utils: Aborting task
org.apache.spark.executor.CommitDeniedException: Commit denied for partition 76 (task 19653, attempt 0, stage 293.0)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:47 ERROR DataWritingSparkTask: Aborting commit for partition 76 (task 19653, attempt 0, stage 293.0)
22/05/07 16:29:47 ERROR DataWritingSparkTask: Aborted commit for partition 76 (task 19653, attempt 0, stage 293.0)
22/05/07 16:29:47 ERROR Inbox: Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@ed1e328 rejected from java.util.concurrent.ThreadPoolExecutor@551f709f[Shutting down, pool size = 5, active threads = 5, queued tasks = 0, completed tasks = 19657]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:47 ERROR Utils: Aborting task
org.apache.spark.executor.CommitDeniedException: Commit denied for partition 80 (task 19657, attempt 0, stage 293.0)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:47 ERROR DataWritingSparkTask: Aborting commit for partition 80 (task 19657, attempt 0, stage 293.0)
22/05/07 16:29:47 ERROR DataWritingSparkTask: Aborted commit for partition 80 (task 19657, attempt 0, stage 293.0)
22/05/07 16:29:47 ERROR Inbox: Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@4d197e0f rejected from java.util.concurrent.ThreadPoolExecutor@551f709f[Shutting down, pool size = 4, active threads = 4, queued tasks = 0, completed tasks = 19658]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:47 ERROR MicroBatchExecution: Query [id = 5c0b9c1e-c064-4091-809a-57c907825bc6, runId = 67692fc3-fcf8-496b-bc30-eafac2c17e16] terminated with error
org.apache.spark.SparkException: Writing job aborted
	at org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobAbortedError(QueryExecutionErrors.scala:613)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:386)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:330)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:279)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:290)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)
	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2971)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2971)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:603)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)
Caused by: org.apache.spark.SparkException: Job 97 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1166)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1164)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1164)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2666)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2566)
	at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2086)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1442)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2086)
	at org.apache.spark.SparkContext.$anonfun$new$38(SparkContext.scala:667)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:354)
	... 40 more
22/05/07 16:29:47 ERROR OutputCommitCoordinator: canCommit called after coordinator was stopped (is SparkEnv shutdown in progress)?
22/05/07 16:29:47 ERROR Utils: Aborting task
org.apache.spark.executor.CommitDeniedException: Commit denied for partition 81 (task 19658, attempt 0, stage 293.0)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:47 ERROR DataWritingSparkTask: Aborting commit for partition 81 (task 19658, attempt 0, stage 293.0)
22/05/07 16:29:47 ERROR DataWritingSparkTask: Aborted commit for partition 81 (task 19658, attempt 0, stage 293.0)
22/05/07 16:29:47 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@77f04dbd rejected from java.util.concurrent.ScheduledThreadPoolExecutor@5fc11cb4[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
22/05/07 16:29:47 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@583dd211 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@5fc11cb4[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
Exception in thread "stream execution thread for [id = 5c0b9c1e-c064-4091-809a-57c907825bc6, runId = 67692fc3-fcf8-496b-bc30-eafac2c17e16]" org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
	at org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)
	at org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:402)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:352)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:333)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)
Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)
	at org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)
	... 8 more
22/05/07 16:29:47 WARN StateStore: Error managing HDFSStateStoreProvider[id = (op=0,part=129),dir = file:/tmp/kafkacheckpoint/state/0/129/right-keyToNumValues], stopping management thread
22/05/07 16:29:47 WARN StateStore: Error running maintenance thread
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
	at org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef$.forExecutor(StateStoreCoordinator.scala:84)
	at org.apache.spark.sql.execution.streaming.state.StateStore$.coordinatorRef(StateStore.scala:648)
	at org.apache.spark.sql.execution.streaming.state.StateStore$.verifyIfStoreInstanceActive(StateStore.scala:630)
	at org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$doMaintenance$2(StateStore.scala:595)
	at org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$doMaintenance$2$adapted(StateStore.scala:593)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.execution.streaming.state.StateStore$.doMaintenance(StateStore.scala:593)
	at org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$startMaintenanceIfNeeded$1(StateStore.scala:577)
	at org.apache.spark.sql.execution.streaming.state.StateStore$MaintenanceTask$$anon$1.run(StateStore.scala:442)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)
	at org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)
	at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:74)
	at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:144)
	... 21 more
22/05/07 16:29:47 ERROR Utils: Aborting task
java.lang.NullPointerException
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:425)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:47 ERROR DataWritingSparkTask: Aborting commit for partition 82 (task 19659, attempt 0, stage 293.0)
22/05/07 16:29:47 ERROR DataWritingSparkTask: Aborted commit for partition 82 (task 19659, attempt 0, stage 293.0)
22/05/07 16:29:47 ERROR Utils: Uncaught exception in thread Executor task launch worker for task 82.0 in stage 293.0 (TID 19659)
java.lang.NullPointerException
	at org.apache.spark.scheduler.Task.$anonfun$run$2(Task.scala:152)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1442)
	at org.apache.spark.scheduler.Task.run(Task.scala:150)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:47 ERROR Executor: Exception in task 82.0 in stage 293.0 (TID 19659): null
22/05/07 16:29:47 ERROR Utils: Aborting task
java.lang.NullPointerException
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:425)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:47 ERROR DataWritingSparkTask: Aborting commit for partition 83 (task 19660, attempt 0, stage 293.0)
22/05/07 16:29:47 ERROR DataWritingSparkTask: Aborted commit for partition 83 (task 19660, attempt 0, stage 293.0)
22/05/07 16:29:47 ERROR Utils: Uncaught exception in thread Executor task launch worker for task 83.0 in stage 293.0 (TID 19660)
java.lang.NullPointerException
	at org.apache.spark.scheduler.Task.$anonfun$run$2(Task.scala:152)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1442)
	at org.apache.spark.scheduler.Task.run(Task.scala:150)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/05/07 16:29:47 ERROR Executor: Exception in task 83.0 in stage 293.0 (TID 19660): null
